{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee62e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    \"host\": os.getenv(\"PGHOST\", \"voz-publica2.postgres.database.azure.com\"),\n",
    "    \"port\": int(os.getenv(\"PGPORT\", 5432)),\n",
    "    \"dbname\": os.getenv(\"PGDATABASE\", \"postgres\"),\n",
    "    \"user\": os.getenv(\"PGUSER\", \"diegomancera\"),\n",
    "    \"password\": os.getenv(\"PGPASSWORD\"),\n",
    "    \"sslmode\": \"require\"\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**DB_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81e213e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_696\\2166641679.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9622 rows from last quarter\n",
      "Date range: 2025-10-01 14:59:00+00:00 to 2025-12-17 23:44:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>speaker_normalized</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "      <th>token_count</th>\n",
       "      <th>published_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>None</td>\n",
       "      <td>La acompa√±an: La jefa de Gobierno de la Ciudad...</td>\n",
       "      <td>[0.027068669,-0.007255755,0.01948871,0.0156713...</td>\n",
       "      <td>22</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>Claudia Sheinbaum Pardo</td>\n",
       "      <td>aci√≥n‚Äù. Y lo tercero, que es hermoso, es regal...</td>\n",
       "      <td>[0.012905889,0.032169674,-0.0027023687,0.00900...</td>\n",
       "      <td>138</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>Claudia Sheinbaum Pardo</td>\n",
       "      <td>viendo, en donde hac√≠a tres reflexiones que le...</td>\n",
       "      <td>[0.028016198,0.020888483,-0.019381993,0.028196...</td>\n",
       "      <td>300</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>Claudia Sheinbaum Pardo</td>\n",
       "      <td>con los estudiantes en la explanada de Ciudad...</td>\n",
       "      <td>[0.0053103263,-0.014485596,4.312076e-06,0.0177...</td>\n",
       "      <td>300</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>Claudia Sheinbaum Pardo</td>\n",
       "      <td>Buenas tardes. Me da gusto estar con ustedes. ...</td>\n",
       "      <td>[0.043162066,-0.018256737,-0.008310742,0.03373...</td>\n",
       "      <td>300</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  doc_id       speaker_normalized  \\\n",
       "0  2025-12-17-conference                     None   \n",
       "1  2025-12-17-conference  Claudia Sheinbaum Pardo   \n",
       "2  2025-12-17-conference  Claudia Sheinbaum Pardo   \n",
       "3  2025-12-17-conference  Claudia Sheinbaum Pardo   \n",
       "4  2025-12-17-conference  Claudia Sheinbaum Pardo   \n",
       "\n",
       "                                                text  \\\n",
       "0  La acompa√±an: La jefa de Gobierno de la Ciudad...   \n",
       "1  aci√≥n‚Äù. Y lo tercero, que es hermoso, es regal...   \n",
       "2  viendo, en donde hac√≠a tres reflexiones que le...   \n",
       "3   con los estudiantes en la explanada de Ciudad...   \n",
       "4  Buenas tardes. Me da gusto estar con ustedes. ...   \n",
       "\n",
       "                                           embedding  token_count  \\\n",
       "0  [0.027068669,-0.007255755,0.01948871,0.0156713...           22   \n",
       "1  [0.012905889,0.032169674,-0.0027023687,0.00900...          138   \n",
       "2  [0.028016198,0.020888483,-0.019381993,0.028196...          300   \n",
       "3  [0.0053103263,-0.014485596,4.312076e-06,0.0177...          300   \n",
       "4  [0.043162066,-0.018256737,-0.008310742,0.03373...          300   \n",
       "\n",
       "               published_at  \n",
       "0 2025-12-17 23:44:00+00:00  \n",
       "1 2025-12-17 23:44:00+00:00  \n",
       "2 2025-12-17 23:44:00+00:00  \n",
       "3 2025-12-17 23:44:00+00:00  \n",
       "4 2025-12-17 23:44:00+00:00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch Embeddings and metadata - Last Quarter Only\n",
    "sql = \"\"\"\n",
    "    SELECT\n",
    "    s.doc_id,\n",
    "    s.speaker_normalized,\n",
    "    s.text,\n",
    "    s.embedding,\n",
    "    s.token_count,\n",
    "    m.published_at\n",
    "    FROM speech_turns s\n",
    "    JOIN raw_transcripts_meta m\n",
    "    ON s.doc_id = m.doc_id\n",
    "    WHERE\n",
    "    s.embedding IS NOT NULL\n",
    "    AND s.text IS NOT NULL\n",
    "    AND length(s.text) > 20\n",
    "    AND s.token_count > 15\n",
    "    AND m.published_at >= '2025-10-01'  -- Last quarter: Oct-Dec 2025\n",
    "    ORDER BY m.published_at DESC;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(sql, conn)\n",
    "print(f\"Loaded {df.shape[0]} rows from last quarter\")\n",
    "print(f\"Date range: {df['published_at'].min()} to {df['published_at'].max()}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b7786",
   "metadata": {},
   "source": [
    "## Convert embbedings to matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dedcbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9622, 1536)\n"
     ]
    }
   ],
   "source": [
    "def parse_embedding(e):\n",
    "    if isinstance(e, list):\n",
    "        return np.array(e, dtype=np.float32)\n",
    "    if isinstance(e, str):\n",
    "        return np.array(json.loads(e), dtype=np.float32)\n",
    "    return np.array(e, dtype=np.float32)\n",
    "\n",
    "embeddings = np.vstack(df[\"embedding\"].apply(parse_embedding).values)\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c8fde",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cfcdcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\Documents\\vozpublica\\.venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9622, 25)\n"
     ]
    }
   ],
   "source": [
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=25,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "embeddings_reduced = reducer.fit_transform(embeddings)\n",
    "print(embeddings_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c0db98",
   "metadata": {},
   "source": [
    "## Clustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54220e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_id\n",
       "-1     2384\n",
       " 18     792\n",
       " 20     548\n",
       " 16     520\n",
       " 19     442\n",
       " 12     405\n",
       " 17     299\n",
       " 24     269\n",
       " 30     216\n",
       " 37     207\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=30,\n",
    "    min_samples=10,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\"\n",
    ")\n",
    "\n",
    "labels = clusterer.fit_predict(embeddings_reduced)\n",
    "\n",
    "df[\"topic_id\"] = labels\n",
    "df[\"topic_id\"].value_counts().head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ee16849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 63\n"
     ]
    }
   ],
   "source": [
    "# Topic filtering \n",
    "topics_df = df[df[\"topic_id\"] != -1].copy()\n",
    "\n",
    "print(\"Number of topics:\", topics_df[\"topic_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "929953dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrtact representative sentences per topic\n",
    "# We‚Äôll select sentences closest to the topic centroid.\n",
    "topic_descriptors = []\n",
    "\n",
    "for topic_id, group in topics_df.groupby(\"topic_id\"):\n",
    "    topic_embeddings = np.vstack(group[\"embedding\"].apply(parse_embedding))\n",
    "    centroid = topic_embeddings.mean(axis=0, keepdims=True)\n",
    "\n",
    "    sims = cosine_similarity(topic_embeddings, centroid).flatten()\n",
    "    top_idx = sims.argsort()[-5:][::-1]\n",
    "\n",
    "    reps = group.iloc[top_idx][[\"text\", \"speaker_normalized\", \"published_at\"]]\n",
    "\n",
    "    topic_descriptors.append({\n",
    "        \"topic_id\": topic_id,\n",
    "        \"size\": len(group),\n",
    "        \"representative_sentences\": reps.to_dict(orient=\"records\")\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5cf6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract keywords per topic (TF-IDF)\n",
    "\n",
    "# Common Spanish stop words\n",
    "spanish_stopwords = [\n",
    "    'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'ser', 'se', 'no', 'haber',\n",
    "    'por', 'con', 'su', 'para', 'como', 'estar', 'tener', 'le', 'lo', 'todo',\n",
    "    'pero', 'm√°s', 'hacer', 'o', 'poder', 'decir', 'este', 'ir', 'otro', 'ese',\n",
    "    'la', 'si', 'me', 'ya', 'ver', 'porque', 'dar', 'cuando', '√©l', 'muy',\n",
    "    'sin', 'vez', 'mucho', 'saber', 'qu√©', 'sobre', 'mi', 'alguno', 'mismo',\n",
    "    'yo', 'tambi√©n', 'hasta', 'a√±o', 'dos', 'querer', 'entre', 'as√≠', 'primero',\n",
    "    'desde', 'grande', 'eso', 'ni', 'nos', 'llegar', 'pasar', 'tiempo', 'ella',\n",
    "    's√≠', 'd√≠a', 'uno', 'bien', 'poco', 'deber', 'entonces', 'poner', 'cosa',\n",
    "    'tanto', 'hombre', 'parecer', 'nuestro', 'tan', 'donde', 'ahora', 'parte',\n",
    "    'despu√©s', 'vida', 'quedar', 'siempre', 'creer', 'hablar', 'llevar', 'dejar',\n",
    "    'nada', 'cada', 'seguir', 'menos', 'nuevo', 'encontrar', 'algo', 'solo',\n",
    "    'decir', 'saber', 'sentir', 'tomar', 'mano', 'antes', 'mundo', 'aqu√≠',\n",
    "    'sus', 'les', 'te', 'esta', 'del', 'al', 'los', 'las', 'unos', 'unas', 'd√≠as',\n",
    "    'buenos', 'buen', 'est√°n', 'c√≥mo', 'tardes', 'buenas', 'permiso',\n",
    "   'gracias', 'alegr√≠as', 'da√±os', 'todas', 'todos', 'estamos', 'chavas', 'chavos'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words=spanish_stopwords,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "topic_keywords = {}\n",
    "\n",
    "for topic_id, group in topics_df.groupby(\"topic_id\"):\n",
    "    texts = group[\"text\"].tolist()\n",
    "    tfidf = vectorizer.fit_transform(texts)\n",
    "    scores = tfidf.mean(axis=0).A1\n",
    "    terms = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    top_terms = terms[scores.argsort()[-10:][::-1]]\n",
    "    topic_keywords[topic_id] = top_terms.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16ec3fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'topic_id': 0,\n",
       "  'size': 34,\n",
       "  'keywords': ['dirige presidenta',\n",
       "   'mensaje dirige',\n",
       "   'escuchemos mensaje',\n",
       "   'escuchemos',\n",
       "   'dirige',\n",
       "   'mensaje',\n",
       "   'mexicanos doctora',\n",
       "   'constitucional',\n",
       "   'presidenta constitucional',\n",
       "   'constitucional estados'],\n",
       "  'examples': [{'text': 'Estimado p√∫blico, escuchemos el mensaje que nos dirige la Presidenta Constitucional de los Estados Unidos Mexicanos, la Doctora Claudia Sheinbaum Pardo.',\n",
       "    'speaker_normalized': None,\n",
       "    'published_at': Timestamp('2025-12-13 19:33:00+0000', tz='UTC')},\n",
       "   {'text': 'Estimado p√∫blico, escuchemos el mensaje que nos dirige la Presidenta Constitucional de los Estados Unidos Mexicanos, la Doctora Claudia Sheinbaum Pardo.',\n",
       "    'speaker_normalized': None,\n",
       "    'published_at': Timestamp('2025-12-14 01:44:00+0000', tz='UTC')},\n",
       "   {'text': 'Estimado p√∫blico, escuchemos el mensaje que nos dirige la Presidenta Constitucional de los Estados Unidos Mexicanos, Doctora Claudia Sheinbaum Pardo.',\n",
       "    'speaker_normalized': None,\n",
       "    'published_at': Timestamp('2025-11-04 19:44:00+0000', tz='UTC')},\n",
       "   {'text': 'Estimado p√∫blico, escuchemos el mensaje que nos dirige la Presidenta Constitucional de los Estados Unidos Mexicanos, Doctora Claudia Sheinbaum Pardo.',\n",
       "    'speaker_normalized': None,\n",
       "    'published_at': Timestamp('2025-10-25 21:04:00+0000', tz='UTC')},\n",
       "   {'text': 'Estimado p√∫blico, escuchemos el mensaje que nos dirige la Presidenta Constitucional de los Estados Unidos Mexicanos, Doctora Claudia Sheinbaum Pardo.',\n",
       "    'speaker_normalized': None,\n",
       "    'published_at': Timestamp('2025-11-06 21:42:00+0000', tz='UTC')}]},\n",
       " {'topic_id': 1,\n",
       "  'size': 33,\n",
       "  'keywords': ['viernes',\n",
       "   'es',\n",
       "   'mexicano',\n",
       "   'viernes mexicano',\n",
       "   'fin',\n",
       "   'negocios',\n",
       "   'una',\n",
       "   'comercio',\n",
       "   'hecho',\n",
       "   'local'],\n",
       "  'examples': [{'text': 'ias, que sigan buscando este distintivo, escaneen el c√≥digo QR o entren a la p√°gina viernesmuymexicano.com.mx y que elijan viernes √∫ltimo de cada mes, los negocios que est√°n afiliados o est√°n registrados. Hoy es viernes, hoy toca. Y tambi√©n a los negocios familiares, que se registren desde hoy. Miren, no est√°n solos, estamos con ustedes. Y estamos ayudando a que incluso los que est√°n en la informalidad se puedan integrar a la formalidad a trav√©s de la transformaci√≥n digital. A los gobiernos estatales y municipales, bueno, pues un agradecimiento, porque ha habido un acompa√±amiento cercano en estas primeras compras, en la difusi√≥n y, sobre todo, que han sido ejemplo de que el servicio p√∫blico tambi√©n impulsa al mercado interno. Y en paralelo, desde la Asamblea Nacional de Empresas Familiares G-32 vamos a seguir articulando a las empresas y negocios familiares para que siga siendo esta pol√≠tica de pa√≠s, esta de consumo local todos los meses, de cr√©dito, capacitaci√≥n en territorio. Y, obviamente, con una medici√≥n p√∫blica para que los resultados se est√©n dando cada viernes y cada viernes cuente. Presidenta: Usted lo dijo y estamos convencidos: ‚ÄúM√©xico',\n",
       "    'speaker_normalized': None,\n",
       "    'published_at': Timestamp('2025-10-03 15:16:00+0000', tz='UTC')},\n",
       "   {'text': ' de Empresas y Negocios Familiares G-32, porque escuchar al pa√≠s real, pues, nos permite construir desde lo local, desde la base, desde el territorio donde est√°n los negocios. Y tambi√©n, ‚ÄúViernes Muy Mexicano‚Äù, como una plataforma que promueve el consumo local y comunitario todo el a√±o y que se potencia en este Buen Fin. Recuerden, lo que funciona cada viernes tambi√©n lo multiplica este fin de semana, porque no hay nada m√°s poderoso que el comercio de barrio con esp√≠ritu muy mexicano. Y recuerden tambi√©n: los negocios que se han registrado en el ‚ÄúViernes Muy Mexicano‚Äù y en ‚ÄúLa Gran Escapada del Buen Fin del Turismo‚Äù tambi√©n tendr√°n la oportunidad de participar en este Buen Fin. Tenemos dos videos: Uno que nos recuerda las primeras fiestas comerciales del Buen Fin, que hac√≠amos en las C√°maras de la Frontera, en la C√°mara de la Frontera, de Saltillo, en Ciudad Ju√°rez, en Mexicali, Tijuana, Matamoros, Reynosa, Nogales, por mencionar algunas de las ciudades. Y tenemos nuestro video tradicional. ‚ÄïAdelante, por favor‚Äï.',\n",
       "    'speaker_normalized': 'Claudia Sheinbaum Pardo',\n",
       "    'published_at': Timestamp('2025-11-06 16:52:00+0000', tz='UTC')},\n",
       "   {'text': 'FINALIZA VIDEO PRESIDENTE DE LA CONFEDERACI√ìN DE C√ÅMARAS NACIONALES DE COMERCIO, SERVICIOS Y TURISMO (CONCANACO SERVYTUR), OCTAVIO DE LA TORRE DE ST√âFFANO: Es parte de lo que se subraya en nuestras familias que est√°n apoyando este impulso. Quiero tambi√©n subrayar algo importante: esta no es solamente una campa√±a de descuentos, esto es econom√≠a con rostro humano. Cuando nosotros elegimos un restaurante de barrio, una ferreter√≠a, un local o una est√©tica en nuestra colonia, lo que estamos haciendo es financiar las becas familiares, estamos ayudando a que se paguen las rentas dignas, estamos sosteniendo aprendizajes, estamos generando y manteniendo los oficios y, sobre todo, evitando que el ingreso se fugue fuera de la comunidad. Eso para nosotros es soberan√≠a cotidiana, es donde decidimos que el circulante se debe de quedar en nuestro pa√≠s. ¬øY qu√© sigue? Tres llamados: El primero, a las familias, que sigan buscando este distintivo, escaneen el c√≥digo QR o entren a la p√°gina viernesmuymexicano.com.mx y que elijan viernes √∫ltimo de cada mes, los negocios que est√°n afili',\n",
       "    'speaker_normalized': None,\n",
       "    'published_at': Timestamp('2025-10-03 15:16:00+0000', tz='UTC')},\n",
       "   {'text': ' por muchos m√°s. Son comercios de todos los rubros que se han sumado con promociones y experiencias que atraen a m√°s y m√°s familias a comprar en su comunidad. Cada √∫ltimo viernes de mes estamos viviendo jornadas de consumo local. Cada negocio participante cuenta con un distintivo y un c√≥digo QR para que las familias ubiquen f√°cilmente en d√≥nde comprar con atractivos descuentos y promociones en los ‚ÄúViernes muy mexicano‚Äù. Esto significa, m√°s clientes, m√°s empleo, m√°s arraigo para quienes sostienen nuestra econom√≠a. Y para las familias representa mejores precios, experiencias cercanas y prosperidad compartida. Para nuestro pa√≠s, para M√©xico, representa cuidar lo nuestro y multiplicar la confianza de lo hecho en casa, lo hecho en nuestro pa√≠s. En Quintana Roo, de la mano de su gobierno, Presidenta, estamos listas y listos para seguir ampliando la participaci√≥n de miles de negocios. Invitamos a todas y todos los prestadores de servicios, comercios, artesanas, artesanos, productores, productoras, restaurantes, hoteles, marinas, talleres, salones de belleza, emprendedoras y emprendedores a registrarse y ser parte de esta gran cruzada nacional por la econom√≠a local, porque ‚Äînunca, nunca me cansar√© de decirlo',\n",
       "    'speaker_normalized': 'Claudia Sheinbaum Pardo',\n",
       "    'published_at': Timestamp('2025-10-03 15:16:00+0000', tz='UTC')},\n",
       "   {'text': 'Adelante, Octavio. PRESIDENTE DE LA CONFEDERACI√ìN DE C√ÅMARAS NACIONALES DE COMERCIO, SERVICIOS Y TURISMO (CONCANACO SERVYTUR), OCTAVIO DE LA TORRE DE ST√âFFANO: Muy buenos d√≠as, Presidenta, presentes. Gracias, nuevamente, Presidenta, por abrirnos este espacio. Y primero que nada, felicidades por su primer a√±o de gobierno. Hace un mes aqu√≠ presentamos ‚Äîen \"La ma√±anera del pueblo\"‚Äî ‚ÄúViernes muy mexicano‚Äù, con un prop√≥sito claro y profundo: que cada familia elija lo nuestro y que cada compra se transforme en orgullo que se compra, en empleo que permanece y en comunidad que se fortalece. Hoy, como cada viernes, volvemos a decir: Hoy toca M√©xico. Y con ello, queremos compartir algunos resultados que vale la pena resaltar. Los resultados de la primera jornada del pasado 26 de septiembre son alentadores: Participaron 23 mil 335 negocios familiares en todo el pa√≠s en la plataforma oficial, es decir, en las redes sociales. Se distribuyeron 250 mil engomados oficiales. Hubo presencia en las 32 entidades federativas. Y 18 entidades federativas realizaron activaciones con primeras compras,',\n",
       "    'speaker_normalized': 'Claudia Sheinbaum Pardo',\n",
       "    'published_at': Timestamp('2025-10-03 15:16:00+0000', tz='UTC')}]}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Topic Summary\n",
    "final_topics = []\n",
    "\n",
    "for t in topic_descriptors:\n",
    "    tid = t[\"topic_id\"]\n",
    "    final_topics.append({\n",
    "        \"topic_id\": tid,\n",
    "        \"size\": t[\"size\"],\n",
    "        \"keywords\": topic_keywords.get(tid, []),\n",
    "        \"examples\": t[\"representative_sentences\"]\n",
    "    })\n",
    "\n",
    "final_topics[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f15816",
   "metadata": {},
   "source": [
    "## Improvements Needed\n",
    "\n",
    "The current approach captures too much formulaic language. Let's implement:\n",
    "1. **Higher token threshold** - Focus on substantive speeches (50+ tokens)\n",
    "2. **Better stop words** - Add domain-specific terms\n",
    "3. **Text preprocessing** - Remove formulaic openings\n",
    "4. **Adjusted UMAP/HDBSCAN** - Better parameters\n",
    "5. **Filter very short or repetitive content**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d0e0766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_696\\3234573980.py:21: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_improved = pd.read_sql(sql_improved, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7391 substantive speeches\n",
      "Token count stats: min=51, mean=212, max=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>speaker_normalized</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "      <th>token_count</th>\n",
       "      <th>published_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>Claudia Sheinbaum Pardo</td>\n",
       "      <td>aci√≥n‚Äù. Y lo tercero, que es hermoso, es regal...</td>\n",
       "      <td>[0.012905889,0.032169674,-0.0027023687,0.00900...</td>\n",
       "      <td>138</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>Claudia Sheinbaum Pardo</td>\n",
       "      <td>viendo, en donde hac√≠a tres reflexiones que le...</td>\n",
       "      <td>[0.028016198,0.020888483,-0.019381993,0.028196...</td>\n",
       "      <td>300</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>Claudia Sheinbaum Pardo</td>\n",
       "      <td>con los estudiantes en la explanada de Ciudad...</td>\n",
       "      <td>[0.0053103263,-0.014485596,4.312076e-06,0.0177...</td>\n",
       "      <td>300</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>Claudia Sheinbaum Pardo</td>\n",
       "      <td>Buenas tardes. Me da gusto estar con ustedes. ...</td>\n",
       "      <td>[0.043162066,-0.018256737,-0.008310742,0.03373...</td>\n",
       "      <td>300</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-17-conference</td>\n",
       "      <td>None</td>\n",
       "      <td>Encabeza esta entrega gratuita de libros ‚Äú25 p...</td>\n",
       "      <td>[0.045315072,0.04872283,0.016775096,0.03860097...</td>\n",
       "      <td>51</td>\n",
       "      <td>2025-12-17 23:44:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  doc_id       speaker_normalized  \\\n",
       "0  2025-12-17-conference  Claudia Sheinbaum Pardo   \n",
       "1  2025-12-17-conference  Claudia Sheinbaum Pardo   \n",
       "2  2025-12-17-conference  Claudia Sheinbaum Pardo   \n",
       "3  2025-12-17-conference  Claudia Sheinbaum Pardo   \n",
       "4  2025-12-17-conference                     None   \n",
       "\n",
       "                                                text  \\\n",
       "0  aci√≥n‚Äù. Y lo tercero, que es hermoso, es regal...   \n",
       "1  viendo, en donde hac√≠a tres reflexiones que le...   \n",
       "2   con los estudiantes en la explanada de Ciudad...   \n",
       "3  Buenas tardes. Me da gusto estar con ustedes. ...   \n",
       "4  Encabeza esta entrega gratuita de libros ‚Äú25 p...   \n",
       "\n",
       "                                           embedding  token_count  \\\n",
       "0  [0.012905889,0.032169674,-0.0027023687,0.00900...          138   \n",
       "1  [0.028016198,0.020888483,-0.019381993,0.028196...          300   \n",
       "2  [0.0053103263,-0.014485596,4.312076e-06,0.0177...          300   \n",
       "3  [0.043162066,-0.018256737,-0.008310742,0.03373...          300   \n",
       "4  [0.045315072,0.04872283,0.016775096,0.03860097...           51   \n",
       "\n",
       "               published_at  \n",
       "0 2025-12-17 23:44:00+00:00  \n",
       "1 2025-12-17 23:44:00+00:00  \n",
       "2 2025-12-17 23:44:00+00:00  \n",
       "3 2025-12-17 23:44:00+00:00  \n",
       "4 2025-12-17 23:44:00+00:00  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPROVED VERSION - Fetch only substantive content\n",
    "sql_improved = \"\"\"\n",
    "    SELECT\n",
    "    s.doc_id,\n",
    "    s.speaker_normalized,\n",
    "    s.text,\n",
    "    s.embedding,\n",
    "    s.token_count,\n",
    "    m.published_at\n",
    "    FROM speech_turns s\n",
    "    JOIN raw_transcripts_meta m\n",
    "    ON s.doc_id = m.doc_id\n",
    "    WHERE\n",
    "    s.embedding IS NOT NULL\n",
    "    AND s.text IS NOT NULL\n",
    "    AND s.token_count > 50  -- Substantive speeches only\n",
    "    AND m.published_at >= '2025-10-01'\n",
    "    ORDER BY m.published_at DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_improved = pd.read_sql(sql_improved, conn)\n",
    "print(f\"Loaded {df_improved.shape[0]} substantive speeches\")\n",
    "print(f\"Token count stats: min={df_improved['token_count'].min()}, mean={df_improved['token_count'].mean():.0f}, max={df_improved['token_count'].max()}\")\n",
    "df_improved.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f62a0eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (7391, 1536)\n"
     ]
    }
   ],
   "source": [
    "# Convert embeddings to matrix\n",
    "embeddings_improved = np.vstack(df_improved[\"embedding\"].apply(parse_embedding).values)\n",
    "print(f\"Embeddings shape: {embeddings_improved.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d11410ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\Documents\\vozpublica\\.venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced shape: (7391, 15)\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED UMAP - Fewer components for better clustering\n",
    "reducer_improved = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=15,  # Reduced from 25\n",
    "    metric=\"cosine\",\n",
    "    min_dist=0.0,  # Allow tighter clusters\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "embeddings_reduced_improved = reducer_improved.fit_transform(embeddings_improved)\n",
    "print(f\"Reduced shape: {embeddings_reduced_improved.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0d87ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics found: 12\n",
      "Noise ratio: 2.8%\n",
      "\n",
      "Topic distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_id\n",
       " 11    3102\n",
       " 8     1203\n",
       " 2      645\n",
       " 10     567\n",
       " 4      480\n",
       " 9      392\n",
       " 7      296\n",
       "-1      206\n",
       " 5      146\n",
       " 6      139\n",
       " 0       91\n",
       " 3       63\n",
       " 1       61\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPROVED HDBSCAN - Adjusted parameters\n",
    "clusterer_improved = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=50,  # Increased from 30 for more coherent topics\n",
    "    min_samples=15,  # Increased from 10\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    cluster_selection_epsilon=0.5  # Allow merging similar clusters\n",
    ")\n",
    "\n",
    "labels_improved = clusterer_improved.fit_predict(embeddings_reduced_improved)\n",
    "df_improved[\"topic_id\"] = labels_improved\n",
    "\n",
    "print(f\"Number of topics found: {len(set(labels_improved)) - (1 if -1 in labels_improved else 0)}\")\n",
    "print(f\"Noise ratio: {(labels_improved == -1).sum() / len(labels_improved):.1%}\")\n",
    "print(\"\\nTopic distribution:\")\n",
    "df_improved[\"topic_id\"].value_counts().head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "379776d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics with assigned clusters: 7185 speeches\n",
      "Number of unique topics: 12\n"
     ]
    }
   ],
   "source": [
    "# Filter out noise\n",
    "topics_df_improved = df_improved[df_improved[\"topic_id\"] != -1].copy()\n",
    "print(f\"Topics with assigned clusters: {topics_df_improved.shape[0]} speeches\")\n",
    "print(f\"Number of unique topics: {topics_df_improved['topic_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c08e055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords for 12 topics\n",
      "  ‚Ä¢ 1 macro-topics (>1500 speeches - narrative backbones)\n",
      "  ‚Ä¢ 11 regular topics\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED: Centroid-based keyword extraction (better than pure TF-IDF)\n",
    "# This aligns descriptors with semantic center, not just frequency\n",
    "\n",
    "spanish_stopwords_expanded = [\n",
    "    # Basic stop words\n",
    "    'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'ser', 'se', 'no', 'haber',\n",
    "    'por', 'con', 'su', 'para', 'como', 'estar', 'tener', 'le', 'lo', 'todo',\n",
    "    'pero', 'm√°s', 'hacer', 'o', 'poder', 'decir', 'este', 'ir', 'otro', 'ese',\n",
    "    'si', 'me', 'ya', 'ver', 'porque', 'dar', 'cuando', '√©l', 'muy', 'sin',\n",
    "    'vez', 'mucho', 'saber', 'qu√©', 'sobre', 'mi', 'alguno', 'mismo', 'yo',\n",
    "    'tambi√©n', 'hasta', 'a√±o', 'dos', 'querer', 'entre', 'as√≠', 'primero',\n",
    "    'desde', 'grande', 'eso', 'ni', 'nos', 'llegar', 'pasar', 'tiempo', 'ella',\n",
    "    's√≠', 'd√≠a', 'uno', 'bien', 'poco', 'deber', 'entonces', 'poner', 'cosa',\n",
    "    'tanto', 'hombre', 'parecer', 'nuestro', 'tan', 'donde', 'ahora', 'parte',\n",
    "    'despu√©s', 'vida', 'quedar', 'siempre', 'creer', 'hablar', 'llevar', 'dejar',\n",
    "    'nada', 'cada', 'seguir', 'menos', 'nuevo', 'encontrar', 'algo', 'solo',\n",
    "    'sentir', 'tomar', 'mano', 'antes', 'mundo', 'aqu√≠', 'sus', 'les', 'te',\n",
    "    'esta', 'del', 'al', 'los', 'las', 'unos', 'unas', 'es', 'una', 'hay', 'est√°', 'ha', 'vamos', 'tenemos',\n",
    "    # Government/formulaic terms\n",
    "    'd√≠as', 'buenos', 'buen', 'est√°n', 'c√≥mo', 'tardes', 'buenas', 'permiso',\n",
    "    'gracias', 'alegr√≠as', 'da√±os', 'todas', 'todos', 'estamos', 'chavas', 'chavos',\n",
    "    'presidenta', 'presidente', 'constitucional', 'estados', 'unidos', 'mexicanos',\n",
    "    'doctora', 'doctor', 'mensaje', 'escuchemos', 'dirige', 'estimado', 'p√∫blico',\n",
    "    'se√±or', 'se√±ora', 'licenciado', 'licenciada', 'secretario', 'secretaria',\n",
    "    'excelencia', 'honorable', 'distinguido', 'distinguida', 'compa√±eros', 'compa√±eras',\n",
    "    'bienvenidos', 'bienvenidas', 'saludo', 'saludos', 'presente', 'presentes',\n",
    "    'asistentes', 'ciudadanos', 'ciudadanas', 'estimados', 'estimadas', 'queridos',\n",
    "    'queridas', 'apreciados', 'apreciadas', 'nombre', 'representaci√≥n', 'gobierno',\n",
    "    'federal', 'nacional', 'pardo', 'sheinbaum', 'claudia', 'rep√∫blica', 'm√©xico'\n",
    "]\n",
    "\n",
    "topic_keywords_improved = {}\n",
    "MACRO_TOPIC_THRESHOLD = 1500  # Clusters larger than this are \"macro-topics\"\n",
    "\n",
    "for topic_id, group in topics_df_improved.groupby(\"topic_id\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "    \n",
    "    is_macro_topic = len(group) > MACRO_TOPIC_THRESHOLD\n",
    "    \n",
    "    # Compute cluster centroid embedding\n",
    "    topic_embeddings = np.vstack(group[\"embedding\"].apply(parse_embedding))\n",
    "    centroid = topic_embeddings.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    # Find speeches closest to centroid (semantic core of the topic)\n",
    "    sims = cosine_similarity(topic_embeddings, centroid).flatten()\n",
    "    \n",
    "    # For macro-topics, use more samples; for regular topics, use fewer\n",
    "    n_core_samples = 100 if is_macro_topic else 50\n",
    "    n_core_samples = min(n_core_samples, len(group))\n",
    "    \n",
    "    top_idx = sims.argsort()[-n_core_samples:]\n",
    "    core_texts = group.iloc[top_idx][\"text\"].tolist()\n",
    "    \n",
    "    # Extract keywords from semantic core only (not all documents)\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        stop_words=spanish_stopwords_expanded,\n",
    "        ngram_range=(1, 3) if not is_macro_topic else (1, 2),  # Simpler n-grams for macro-topics\n",
    "        min_df=2,\n",
    "        max_df=0.7\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        tfidf = vectorizer.fit_transform(core_texts)\n",
    "        scores = tfidf.mean(axis=0).A1\n",
    "        terms = np.array(vectorizer.get_feature_names_out())\n",
    "        \n",
    "        # For macro-topics, get more keywords (they're narrative backbones)\n",
    "        n_keywords = 20 if is_macro_topic else 15\n",
    "        top_terms = terms[scores.argsort()[-n_keywords:][::-1]]\n",
    "        \n",
    "        topic_keywords_improved[topic_id] = {\n",
    "            \"keywords\": top_terms.tolist(),\n",
    "            \"is_macro_topic\": is_macro_topic,\n",
    "            \"size\": len(group)\n",
    "        }\n",
    "    except:\n",
    "        # Handle edge cases\n",
    "        topic_keywords_improved[topic_id] = {\n",
    "            \"keywords\": [],\n",
    "            \"is_macro_topic\": is_macro_topic,\n",
    "            \"size\": len(group)\n",
    "        }\n",
    "\n",
    "print(f\"Extracted keywords for {len(topic_keywords_improved)} topics\")\n",
    "macro_count = sum(1 for v in topic_keywords_improved.values() if v[\"is_macro_topic\"])\n",
    "print(f\"  ‚Ä¢ {macro_count} macro-topics (>{MACRO_TOPIC_THRESHOLD} speeches - narrative backbones)\")\n",
    "print(f\"  ‚Ä¢ {len(topic_keywords_improved) - macro_count} regular topics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27a06c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptors created for 12 topics\n"
     ]
    }
   ],
   "source": [
    "# Extract representative sentences for improved topics\n",
    "topic_descriptors_improved = []\n",
    "\n",
    "for topic_id, group in topics_df_improved.groupby(\"topic_id\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "        \n",
    "    topic_embeddings = np.vstack(group[\"embedding\"].apply(parse_embedding))\n",
    "    centroid = topic_embeddings.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    sims = cosine_similarity(topic_embeddings, centroid).flatten()\n",
    "    top_idx = sims.argsort()[-5:][::-1]\n",
    "    \n",
    "    reps = group.iloc[top_idx][[\"text\", \"speaker_normalized\", \"published_at\", \"token_count\"]]\n",
    "    \n",
    "    topic_descriptors_improved.append({\n",
    "        \"topic_id\": topic_id,\n",
    "        \"size\": len(group),\n",
    "        \"representative_sentences\": reps.to_dict(orient=\"records\")\n",
    "    })\n",
    "\n",
    "print(f\"Descriptors created for {len(topic_descriptors_improved)} topics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cde4e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total topics discovered: 12\n",
      "  ‚Ä¢ 1 MACRO-TOPICS (narrative backbones, >1500 speeches)\n",
      "  ‚Ä¢ 11 regular topics\n",
      "\n",
      "================================================================================\n",
      "TOP 5 TOPICS BY SIZE\n",
      "================================================================================\n",
      "\n",
      "üî¥ MACRO-TOPIC\n",
      "üìå TOPIC 11 (3102 speeches)\n",
      "Keywords: seguridad, pueblo, michoac√°n, hemos, muchas, tiene, tema, nosotros\n",
      "\n",
      "Example excerpt:  luchamos contra eso. Entonces, se cae por s√≠ mismo. Que ‚Äúla Presidenta protege a delincuentes‚Äù. Bueno, el Consejo de Seguridad de M√©xico es el √∫nico que ha enviado delincuentes de un lado y del otro,...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìå TOPIC 8 (1203 speeches)\n",
      "Keywords: caminos, mil, trabajando, son, va, estado, hidalgo, puentes\n",
      "\n",
      "Example excerpt:  liberando todo el camino. Est√° all√°, hoy llega el subsecretario de Infraestructura all√°, para apoyar las labores; la subsecretaria de Transportes est√° en Hidalgo; estamos con los directores generales...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìå TOPIC 2 (645 speeches)\n",
      "Keywords: medicamentos, bienestar, atenci√≥n, imss bienestar, va, issste, hospitales, mil\n",
      "\n",
      "Example excerpt:  los d√≠as, mejor infraestructura, telemedicina y siempre, siempre, siempre un trato digno. Son pasos firmes para que cualquier persona, viva donde viva, reciba atenci√≥n gratuita y de calidad. Estamos ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìå TOPIC 10 (567 speeches)\n",
      "Keywords: va, tema, bueno, acuerdo, nosotros, tiene, hemos, seguridad\n",
      "\n",
      "Example excerpt: Bueno, ahora vamos a esperar que llegue Marcelo Ebrard de su viaje, que fue muy intenso. Yo, por lo que he hablado con √©l, ya pr√°cticamente cerramos el tema de las famosas 54 medidas no arancelarias s...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìå TOPIC 4 (480 speeches)\n",
      "Keywords: superior, j√≥venes, mil, universidades, educaci√≥n superior, pa√≠s, plataforma, universidad\n",
      "\n",
      "Example excerpt:  posibilidad para lograrlo. Y no se nos olvida tampoco que, adem√°s de democratizar el acceso al conocimiento, de ofrecer la plataforma de Saberes, tenemos que seguir trabajando en abrir espacios en la...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build improved final topics summary with macro-topic labels\n",
    "final_topics_improved = []\n",
    "\n",
    "for t in topic_descriptors_improved:\n",
    "    tid = t[\"topic_id\"]\n",
    "    topic_info = topic_keywords_improved.get(tid, {\"keywords\": [], \"is_macro_topic\": False, \"size\": 0})\n",
    "    \n",
    "    final_topics_improved.append({\n",
    "        \"topic_id\": tid,\n",
    "        \"size\": t[\"size\"],\n",
    "        \"is_macro_topic\": topic_info[\"is_macro_topic\"],\n",
    "        \"keywords\": topic_info[\"keywords\"][:10],  # Top 10 keywords\n",
    "        \"examples\": t[\"representative_sentences\"][:3]  # Top 3 examples\n",
    "    })\n",
    "\n",
    "# Sort by size (largest topics first)\n",
    "final_topics_improved.sort(key=lambda x: x[\"size\"], reverse=True)\n",
    "\n",
    "print(f\"Total topics discovered: {len(final_topics_improved)}\")\n",
    "\n",
    "# Count macro-topics\n",
    "macro_topics = [t for t in final_topics_improved if t[\"is_macro_topic\"]]\n",
    "regular_topics = [t for t in final_topics_improved if not t[\"is_macro_topic\"]]\n",
    "\n",
    "print(f\"  ‚Ä¢ {len(macro_topics)} MACRO-TOPICS (narrative backbones, >1500 speeches)\")\n",
    "print(f\"  ‚Ä¢ {len(regular_topics)} regular topics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 5 TOPICS BY SIZE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, topic in enumerate(final_topics_improved[:5], 1):\n",
    "    topic_type = \"üî¥ MACRO-TOPIC\" if topic[\"is_macro_topic\"] else \"üü¢ Regular Topic\"\n",
    "    print(f\"\\n{topic_type}\")\n",
    "    print(f\"üìå TOPIC {topic['topic_id']} ({topic['size']} speeches)\")\n",
    "    print(f\"Keywords: {', '.join(topic['keywords'][:8])}\")\n",
    "    print(f\"\\nExample excerpt: {topic['examples'][0]['text'][:200]}...\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c53da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üî¥ MACRO-TOPIC (Narrative Backbone)\n",
      "TOPIC 11 - Size: 3102 speeches\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  This is a macro-topic (>1500 speeches)\n",
      "   ‚Ä¢ Don't expect sharp keywords - it's a broad narrative backbone\n",
      "   ‚Ä¢ Keywords below are from the semantic core (centroid-based)\n",
      "   ‚Ä¢ Consider sub-clustering this topic for finer granularity\n",
      "\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ seguridad\n",
      "   ‚Ä¢ pueblo\n",
      "   ‚Ä¢ michoac√°n\n",
      "   ‚Ä¢ hemos\n",
      "   ‚Ä¢ muchas\n",
      "   ‚Ä¢ tiene\n",
      "   ‚Ä¢ tema\n",
      "   ‚Ä¢ nosotros\n",
      "   ‚Ä¢ son\n",
      "   ‚Ä¢ justicia\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-12-02 15:26:00+00:00\n",
      "   Text:  luchamos contra eso. Entonces, se cae por s√≠ mismo. Que ‚Äúla Presidenta protege a delincuentes‚Äù. Bueno, el Consejo de Seguridad de M√©xico es el √∫nico que ha enviado delincuentes de un lado y del otro, y del otro, y del otro, a Estados Unidos, extraditados o enviados. O sea, cuando pas√≥ eso dec√≠an un...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-11-03 15:32:00+00:00\n",
      "   Text: Es la Estrategia de un Gabinete y de una convicci√≥n, no es la estrategia de ‚Äúsolo la Presidenta‚Äù. No puede haber seguridad donde no hay justicia. Y tenemos que fortalecer las instituciones de procuraci√≥n y de justicia. Es decir, ¬øqu√© es lo que hemos estado proponiendo nosotros? Miren, les voy a pone...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 8 - Size: 1203 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ caminos\n",
      "   ‚Ä¢ mil\n",
      "   ‚Ä¢ trabajando\n",
      "   ‚Ä¢ son\n",
      "   ‚Ä¢ va\n",
      "   ‚Ä¢ estado\n",
      "   ‚Ä¢ hidalgo\n",
      "   ‚Ä¢ puentes\n",
      "   ‚Ä¢ apoyo\n",
      "   ‚Ä¢ veracruz\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (300 tokens):\n",
      "   Speaker: Jes√∫s Antonio Esteva Medina\n",
      "   Date: 2025-10-15 17:08:00+00:00\n",
      "   Text:  liberando todo el camino. Est√° all√°, hoy llega el subsecretario de Infraestructura all√°, para apoyar las labores; la subsecretaria de Transportes est√° en Hidalgo; estamos con los directores generales de Conservaci√≥n y Carreteras, tambi√©n apoyando en los diferentes estados. Entonces, los equipos y l...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-10-15 17:08:00+00:00\n",
      "   Text: Bien, tambi√©n. Bueno, el d√≠a de hoy vamos a seguir informando sobre el avance a la atenci√≥n de emergencia, por las lluvias. Se encuentra con nosotros el General secretario Ricardo Trevilla Trejo, secretario de la Defensa; el Almirante Raymundo Pedro Morales √Ångeles, secretario de Marina; y el secret...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 2 - Size: 645 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ medicamentos\n",
      "   ‚Ä¢ bienestar\n",
      "   ‚Ä¢ atenci√≥n\n",
      "   ‚Ä¢ imss bienestar\n",
      "   ‚Ä¢ va\n",
      "   ‚Ä¢ issste\n",
      "   ‚Ä¢ hospitales\n",
      "   ‚Ä¢ mil\n",
      "   ‚Ä¢ centros\n",
      "   ‚Ä¢ hoy\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (300 tokens):\n",
      "   Speaker: Alejandro Svarch P√©rez\n",
      "   Date: 2025-11-22 20:36:00+00:00\n",
      "   Text:  los d√≠as, mejor infraestructura, telemedicina y siempre, siempre, siempre un trato digno. Son pasos firmes para que cualquier persona, viva donde viva, reciba atenci√≥n gratuita y de calidad. Estamos aqu√≠ para decirles que la salud es un derecho y que el gobierno de la Presidenta Claudia Sheinbaum P...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-12-10 20:57:00+00:00\n",
      "   Text:  dise√±amos el sistema de Rutas de la Salud. Bajo la coordinaci√≥n del doctor Kershenobich dise√±amos los programas, los PRONAMs, los Programas Nacionales de Salud, para poder identificar los mejores medicamentos para las distintas enfermedades. Catalogamos cu√°ntos medicamentos deben estar en el primer...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 10 - Size: 567 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ va\n",
      "   ‚Ä¢ tema\n",
      "   ‚Ä¢ bueno\n",
      "   ‚Ä¢ acuerdo\n",
      "   ‚Ä¢ nosotros\n",
      "   ‚Ä¢ tiene\n",
      "   ‚Ä¢ hemos\n",
      "   ‚Ä¢ seguridad\n",
      "   ‚Ä¢ trump\n",
      "   ‚Ä¢ inversi√≥n\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (159 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-10-31 15:44:00+00:00\n",
      "   Text: Bueno, ahora vamos a esperar que llegue Marcelo Ebrard de su viaje, que fue muy intenso. Yo, por lo que he hablado con √©l, ya pr√°cticamente cerramos el tema de las famosas 54 medidas no arancelarias sin‚Ä¶ poni√©ndonos de acuerdo, o sea, sin‚Ä¶ nosotros siempre defendiendo el inter√©s de M√©xico, obviament...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-10-29 16:04:00+00:00\n",
      "   Text: Pues puede ser un inter√©s pol√≠tico, puede ser un inter√©s de apoyar a algunas empresas frente a otras, incluso estadounidenses. Por eso digo ‚Äúno vaya a ser‚Äù que haya una situaci√≥n de este tipo. Le ped√≠ tambi√©n ayer a Andrea Marv√°n, que es de la nueva Comisi√≥n Nacional Antimonopolio ‚Äîlo que en su mome...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 4 - Size: 480 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ superior\n",
      "   ‚Ä¢ j√≥venes\n",
      "   ‚Ä¢ mil\n",
      "   ‚Ä¢ universidades\n",
      "   ‚Ä¢ educaci√≥n superior\n",
      "   ‚Ä¢ pa√≠s\n",
      "   ‚Ä¢ plataforma\n",
      "   ‚Ä¢ universidad\n",
      "   ‚Ä¢ muchas\n",
      "   ‚Ä¢ escuela\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (234 tokens):\n",
      "   Speaker: Mario Delgado Carrillo\n",
      "   Date: 2025-11-21 16:25:00+00:00\n",
      "   Text:  posibilidad para lograrlo. Y no se nos olvida tampoco que, adem√°s de democratizar el acceso al conocimiento, de ofrecer la plataforma de Saberes, tenemos que seguir trabajando en abrir espacios en las instituciones de educaci√≥n superior. La meta de cobertura de 55 por ciento que estableci√≥ la Presi...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Mario Delgado Carrillo\n",
      "   Date: 2025-11-09 20:25:00+00:00\n",
      "   Text:  ir a los lugares donde los j√≥venes lo est√°n demandando. Tambi√©n se tiene un plan muy ambicioso de lograr 50 mil nuevos lugares en educaci√≥n superior, trabajando en conjunto con la UNAM, con la Universidad Michoacana de San Nicol√°s de Hidalgo, con los 17 Tecnol√≥gicos Nacionales de M√©xico que hay en ...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 9 - Size: 392 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ ley\n",
      "   ‚Ä¢ riego\n",
      "   ‚Ä¢ conagua\n",
      "   ‚Ä¢ aguas\n",
      "   ‚Ä¢ va\n",
      "   ‚Ä¢ esa\n",
      "   ‚Ä¢ importante\n",
      "   ‚Ä¢ tiene\n",
      "   ‚Ä¢ tienen\n",
      "   ‚Ä¢ concesiones\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (239 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-11-25 17:42:00+00:00\n",
      "   Text: , a cambio tienen que entregar una parte del agua que tienen concesionada, ¬øpara qui√©n?, para la gente. 70 por ciento del agua en nuestro pa√≠s se usa en los distritos de riego, casi 80. Entonces, ¬øqu√© estamos haciendo?, que se use menos agua en el campo, que se tecnifique, poniendo recursos a los di...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-11-25 17:42:00+00:00\n",
      "   Text:  Aguas. Y s√≠ tengo que decirlo: hay quien se manifiesta para mantener privilegios que ya no queremos que existan. La Ley de Aguas ‚Äîlo hemos planteado aqu√≠ en varias ocasiones‚Äî tiene el planteamiento de regresar a la naci√≥n o regresar como recurso de la naci√≥n el agua a la ley, y de garantizar el der...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 7 - Size: 296 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ pa√≠s\n",
      "   ‚Ä¢ fifa\n",
      "   ‚Ä¢ futbol\n",
      "   ‚Ä¢ ciudad\n",
      "   ‚Ä¢ van\n",
      "   ‚Ä¢ copa\n",
      "   ‚Ä¢ tres\n",
      "   ‚Ä¢ va\n",
      "   ‚Ä¢ actividades\n",
      "   ‚Ä¢ ciudades\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-11-12 13:52:00+00:00\n",
      "   Text: S√≠. Hay rutas tur√≠sticas vinculadas con la llegada de turistas al Mundial, eso es lo que vamos a presentar, ya sea la pr√≥xima o hasta la otra semana. Hay dos temas que queremos presentar: Uno. Lo que van a ser las ciudades de sedes: la Ciudad de M√©xico, Guadalajara y Monterrey. Que es importante que...\n",
      "\n",
      "   Example 2 (274 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-11-10 17:17:00+00:00\n",
      "   Text: 11 de junio, ya quedan pocos meses. En las siguientes semanas vamos a invitar a la jefa de Gobierno, al gobernador de Jalisco y de Nuevo Le√≥n, para que ellos platiquen lo que est√°n haciendo en las tres entidades. Y tambi√©n vamos a presentar lo que llamamos ‚ÄúEl Mundial Social‚Äù, ‚ÄúEl Mundialito Social‚Äù...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 5 - Size: 146 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ productores\n",
      "   ‚Ä¢ precio\n",
      "   ‚Ä¢ va\n",
      "   ‚Ä¢ mil\n",
      "   ‚Ä¢ programa\n",
      "   ‚Ä¢ apoyo\n",
      "   ‚Ä¢ nosotros\n",
      "   ‚Ä¢ adem√°s\n",
      "   ‚Ä¢ son\n",
      "   ‚Ä¢ tiene\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-11-06 16:52:00+00:00\n",
      "   Text:  manera gratuita a peque√±os productores, principalmente de ma√≠z. Y, adem√°s, est√° el programa Sembrando Vida. Entre los tres programas, Sembrando Vida tiene cerca de 39 mil millones de pesos; Producci√≥n para el Bienestar cerca de 16 mil, si mal no me acuerdo; y alrededor de 17 mil, Fertilizantes [par...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-10-29 16:04:00+00:00\n",
      "   Text: Hablamos de precio justo. Claro que nosotros vivimos en un mercado global y M√©xico tiene‚Ä¶ es de los pa√≠ses que m√°s Tratados Comerciales tiene en el mundo; no vino de la √©poca en que nosotros gobern√°bamos, pero tiene sus ventajas y tambi√©n tiene sus desventajas. ¬øQu√© estamos construyendo particularme...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 6 - Size: 139 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ viviendas\n",
      "   ‚Ä¢ mil viviendas\n",
      "   ‚Ä¢ programa\n",
      "   ‚Ä¢ meta\n",
      "   ‚Ä¢ fovissste\n",
      "   ‚Ä¢ son\n",
      "   ‚Ä¢ importante\n",
      "   ‚Ä¢ personas\n",
      "   ‚Ä¢ hoy\n",
      "   ‚Ä¢ cr√©ditos\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (300 tokens):\n",
      "   Speaker: Unknown\n",
      "   Date: 2025-11-29 17:10:00+00:00\n",
      "   Text:  Ram√≠rez. A nuestras personas beneficiarias, que hay en el pres√≠dium, son puras mujeres, a todas y todas. La verdad es que siempre es un gusto estar ac√°, en esta hermos√≠sima tierra de Quintana Roo, es un gusto, y hoy en particular se suma otro gusto m√°s por todo lo que hoy significa este evento. Qui...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Unknown\n",
      "   Date: 2025-10-06 15:40:00+00:00\n",
      "   Text:  todo el contrato, con la excepci√≥n, se testan los datos que son privados; pero viene todo el contrato con todos los datos. Y as√≠, para todas las viviendas que se est√°n contratando, y esto se va a actualizando peri√≥dicamente, conforme se van contratando las viviendas. Bueno, tenemos otro programa qu...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 0 - Size: 91 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ d√≥lares\n",
      "   ‚Ä¢ centavos\n",
      "   ‚Ä¢ mil\n",
      "   ‚Ä¢ da\n",
      "   ‚Ä¢ barata\n",
      "   ‚Ä¢ cara\n",
      "   ‚Ä¢ palomita cara\n",
      "   ‚Ä¢ precio\n",
      "   ‚Ä¢ promedio\n",
      "   ‚Ä¢ gasolina\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (300 tokens):\n",
      "   Speaker: Iv√°n Escalante Ruiz\n",
      "   Date: 2025-11-17 16:05:00+00:00\n",
      "   Text:  400 d√≥lares. Y aqu√≠ el bloque de las empresas que dan mejor promedio de comisi√≥n y un mejor promedio de tipo de cambio; les ponemos una palomita. Y la que menos da en esta modalidad, Pangea Money Transfer, con 7 mil 81 con 37. Recordarles que en la p√°gina de Internet de Profeco hay una calculadora ...\n",
      "\n",
      "   Example 2 (249 tokens):\n",
      "   Speaker: Iv√°n Escalante Ruiz\n",
      "   Date: 2025-10-27 16:08:00+00:00\n",
      "   Text:  a hacer una invitaci√≥n. Aqu√≠ algunas fotograf√≠as de las brigadas y de los compa√±eros haciendo estos monitoreos de precios y colocando los preciadores para que las personas consumidoras tengan la informaci√≥n. Finalizamos con los Consejos Profeco de la semana: 1. Con Finabien puedes recibir hasta 410...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 3 - Size: 63 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ supercomputadora\n",
      "   ‚Ä¢ datos\n",
      "   ‚Ä¢ centro\n",
      "   ‚Ä¢ superc√≥mputo\n",
      "   ‚Ä¢ va\n",
      "   ‚Ä¢ capacidad\n",
      "   ‚Ä¢ desarrollo\n",
      "   ‚Ä¢ c√≥mputo\n",
      "   ‚Ä¢ pa√≠s\n",
      "   ‚Ä¢ esto\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (300 tokens):\n",
      "   Speaker: Titular De La Agencia De Transformaci√≥n Digital Y Telecomunicaciones\n",
      "   Date: 2025-11-26 13:54:00+00:00\n",
      "   Text: Muchas gracias. Pues, bueno, lleg√≥ el d√≠a de presentar a ‚ÄúCoatlicue‚Äù, uno de los proyectos de innovaci√≥n, tecnolog√≠a, m√°s importantes y que hemos hecho junto con la Secretar√≠a a cargo de Rosaura y, por supuesto, bajo la instrucci√≥n de la Presidenta Claudia Sheinbaum. ‚ÄúCoatlicue‚Äù, la supercomputadora...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Claudia Sheinbaum Pardo\n",
      "   Date: 2025-11-19 15:46:00+00:00\n",
      "   Text: , podr√≠amos analizar los datos mucho mejor y poder predecir mejor, entre otras muchas cosas. En lo que se construye la supercomputadora en nuestro pa√≠s, que va a dar un impulso enorme al desarrollo de M√©xico y a la capacidad cient√≠fica y tecnol√≥gica de nuestro pa√≠s, nos abren la puerta en la superco...\n",
      "\n",
      "================================================================================\n",
      "üü¢ Regular Topic\n",
      "TOPIC 1 - Size: 61 speeches\n",
      "================================================================================\n",
      "üîë Keywords (extracted from semantic centroid):\n",
      "   ‚Ä¢ libro\n",
      "   ‚Ä¢ leer\n",
      "   ‚Ä¢ 25\n",
      "   ‚Ä¢ va\n",
      "   ‚Ä¢ lectura\n",
      "   ‚Ä¢ paco\n",
      "   ‚Ä¢ cultura\n",
      "   ‚Ä¢ son\n",
      "   ‚Ä¢ colecci√≥n\n",
      "   ‚Ä¢ j√≥venes\n",
      "\n",
      "üìù Representative Examples (closest to centroid):\n",
      "\n",
      "   Example 1 (300 tokens):\n",
      "   Speaker: Unknown\n",
      "   Date: 2025-12-17 23:44:00+00:00\n",
      "   Text:  fue a repartir libros en un acto que estaba programado para que lo hiciera el ministro de Cultura. Agradecemos enormemente la intervenci√≥n del presidente Ar√©valo, en Guatemala, muy importante, porque abri√≥ una puerta en unas relaciones fraternas que no acababan de consolidarse. El libro mueve infor...\n",
      "\n",
      "   Example 2 (300 tokens):\n",
      "   Speaker: Director General Del Fondo De Cultura Econ√≥mica\n",
      "   Date: 2025-10-23 14:11:00+00:00\n",
      "   Text:  Hay otros proyectos para regalar miles de libros para ni√±os en diciembre, pero esto va destinado a este p√∫blico. ¬øCu√°l es la expectativa? La expectativa es que cambiemos la manera de leer de millares, decenas de millares, centenares de millares de adolescentes Y adem√°s, no les va a poder tocar los ...\n"
     ]
    }
   ],
   "source": [
    "# Detailed view of all topics with macro-topic indicators\n",
    "for topic in final_topics_improved:\n",
    "    topic_type = \"üî¥ MACRO-TOPIC (Narrative Backbone)\" if topic[\"is_macro_topic\"] else \"üü¢ Regular Topic\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{topic_type}\")\n",
    "    print(f\"TOPIC {topic['topic_id']} - Size: {topic['size']} speeches\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if topic[\"is_macro_topic\"]:\n",
    "        print(\"\\n‚ö†Ô∏è  This is a macro-topic (>1500 speeches)\")\n",
    "        print(\"   ‚Ä¢ Don't expect sharp keywords - it's a broad narrative backbone\")\n",
    "        print(\"   ‚Ä¢ Keywords below are from the semantic core (centroid-based)\")\n",
    "        print(\"   ‚Ä¢ Consider sub-clustering this topic for finer granularity\\n\")\n",
    "    \n",
    "    print(f\"üîë Keywords (extracted from semantic centroid):\")\n",
    "    for kw in topic['keywords'][:10]:\n",
    "        print(f\"   ‚Ä¢ {kw}\")\n",
    "    \n",
    "    print(f\"\\nüìù Representative Examples (closest to centroid):\")\n",
    "    for i, ex in enumerate(topic['examples'][:2], 1):\n",
    "        print(f\"\\n   Example {i} ({ex.get('token_count', 'N/A')} tokens):\")\n",
    "        print(f\"   Speaker: {ex['speaker_normalized'] or 'Unknown'}\")\n",
    "        print(f\"   Date: {ex['published_at']}\")\n",
    "        print(f\"   Text: {ex['text'][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6293847",
   "metadata": {},
   "source": [
    "## Summary of Improvements Applied\n",
    "\n",
    "‚úÖ **Centroid-based keyword extraction** - Keywords are extracted from speeches closest to the cluster centroid (semantic core), not all documents. This gives semantically-aligned descriptors rather than just frequency-based ones.\n",
    "\n",
    "‚úÖ **Macro-topic labeling** - Clusters with >1500 speeches are labeled as \"macro-topics\" (narrative backbones). These don't have sharp keywords by nature and represent broad thematic areas. They can be sub-clustered later for finer granularity.\n",
    "\n",
    "‚úÖ **Higher token threshold (50+)** - Filters out formulaic/ceremonial language\n",
    "\n",
    "‚úÖ **Optimized UMAP/HDBSCAN** - Better clustering parameters for political corpora\n",
    "\n",
    "‚úÖ **Enhanced stop words** - Domain-specific terms filtered out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b5725d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-clustering macro-topic 0 (91 speeches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\Documents\\vozpublica\\.venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sub-topics found: 0\n",
      "Noise: 91 speeches\n",
      "\n",
      "Sub-topic distribution:\n",
      "sub_topic_id\n",
      "-1    91\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Sub-cluster a macro-topic for finer granularity\n",
    "# Pick a macro-topic ID and run this cell to break it down further\n",
    "\n",
    "MACRO_TOPIC_TO_SUBCLUSTER = 0  # Change this to the macro-topic ID you want to split\n",
    "\n",
    "# Filter to just this macro-topic\n",
    "macro_df = df_improved[df_improved[\"topic_id\"] == MACRO_TOPIC_TO_SUBCLUSTER].copy()\n",
    "\n",
    "if len(macro_df) > 0:\n",
    "    print(f\"Sub-clustering macro-topic {MACRO_TOPIC_TO_SUBCLUSTER} ({len(macro_df)} speeches)\")\n",
    "    \n",
    "    # Extract embeddings\n",
    "    macro_embeddings = np.vstack(macro_df[\"embedding\"].apply(parse_embedding).values)\n",
    "    \n",
    "    # Reduce with UMAP\n",
    "    sub_reducer = umap.UMAP(\n",
    "        n_neighbors=15,\n",
    "        n_components=10,\n",
    "        metric=\"cosine\",\n",
    "        min_dist=0.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    macro_reduced = sub_reducer.fit_transform(macro_embeddings)\n",
    "    \n",
    "    # Cluster with HDBSCAN (more granular)\n",
    "    sub_clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=100,  # Smaller clusters\n",
    "        min_samples=20,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\"\n",
    "    )\n",
    "    sub_labels = sub_clusterer.fit_predict(macro_reduced)\n",
    "    \n",
    "    macro_df[\"sub_topic_id\"] = sub_labels\n",
    "    \n",
    "    print(f\"\\nSub-topics found: {len(set(sub_labels)) - (1 if -1 in sub_labels else 0)}\")\n",
    "    print(f\"Noise: {(sub_labels == -1).sum()} speeches\")\n",
    "    print(\"\\nSub-topic distribution:\")\n",
    "    print(macro_df[\"sub_topic_id\"].value_counts().head(10))\n",
    "else:\n",
    "    print(f\"Macro-topic {MACRO_TOPIC_TO_SUBCLUSTER} not found or is not a macro-topic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0d822",
   "metadata": {},
   "source": [
    "## LLM Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1bc69162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topic_prompt(topic):\n",
    "    examples = \"\\n\".join(\n",
    "        f\"- {ex['text'][:300]}\"\n",
    "        for ex in topic[\"examples\"][:5]\n",
    "    )\n",
    "\n",
    "    topic_type = \"macro-topic\" if topic[\"size\"] > 1500 else \"regular topic\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are analyzing political speech topics discovered using semantic clustering.\n",
    "\n",
    "This topic is a {topic_type}.\n",
    "\n",
    "Keywords:\n",
    "{\", \".join(topic[\"keywords\"])}\n",
    "\n",
    "Representative excerpts:\n",
    "{examples}\n",
    "\n",
    "Tasks:\n",
    "1. Provide a concise topic label (3‚Äì6 words).\n",
    "2. Write a brief description (2‚Äì3 sentences) explaining what this topic represents.\n",
    "3. If this is a macro-topic, explain why it functions as a narrative backbone.\n",
    "\n",
    "Do NOT invent information.\n",
    "Base your answer strictly on the provided material.\n",
    "\"\"\"\n",
    "\n",
    "    return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6dd53275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded from .env\n",
      "‚úÖ Azure OpenAI client initialized\n",
      "Using deployment: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "print(\"‚úÖ Environment variables loaded from .env\")\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    ")\n",
    "\n",
    "azure_openai_chat_deployment = os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\", \"gpt-4.1\")\n",
    "\n",
    "def label_topic_with_llm(topic, client, deployment=azure_openai_chat_deployment):\n",
    "    prompt = build_topic_prompt(topic)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a careful political discourse analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"‚úÖ Azure OpenAI client initialized\")\n",
    "print(f\"Using deployment: {azure_openai_chat_deployment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "907651fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LLM labels for top 3 topics...\n",
      "================================================================================\n",
      "\n",
      "üè∑Ô∏è  Labeling Topic 11 (Size: 3102)...\n",
      "\n",
      "1. Topic Label  \n",
      "Security, Justice, and Governance in Michoac√°n\n",
      "\n",
      "2. Brief Description  \n",
      "This topic centers on issues of public security and justice in Michoac√°n, highlighting government strategies to combat crime, strengthen institutions, and address extortion affecting local industries. It emphasizes collective governmental action, extradition of criminals, and the need for robust law enforcement and judicial systems.\n",
      "\n",
      "3. Macro-topic Explanation  \n",
      "As a macro-topic, this functions as a narrative backbone because it integrates core themes of governance, security, and justice that underpin broader political discourse. It connects specific regional concerns (Michoac√°n, extortion, local industries) with national strategies and institutional reforms, serving as a foundation for related discussions on policy, leadership, and public trust.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Labeling Topic 8 (Size: 1203)...\n",
      "\n",
      "1. Topic Label: Road Infrastructure and Emergency Response\n",
      "\n",
      "2. Description: This topic centers on government efforts to restore and maintain roadways and bridges, particularly in response to emergencies such as heavy rains. It highlights coordination among federal and state officials, with specific references to regions like Hidalgo and Veracruz, and emphasizes ongoing work and support for affected areas.\n",
      "\n",
      "3. Macro-topic Status: This is a regular topic, not a macro-topic.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Labeling Topic 2 (Size: 645)...\n",
      "\n",
      "1. Topic Label: Public Healthcare Access and Medication\n",
      "\n",
      "2. Description:  \n",
      "This topic centers on government efforts to improve healthcare infrastructure, ensure access to free and quality medical attention, and rationalize the distribution of essential medications. It highlights initiatives such as telemedicine, national health programs, and outreach to rural and underserved communities.\n",
      "\n",
      "3. Macro-topic Explanation:  \n",
      "This is a regular topic, not a macro-topic.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úÖ Test labeling complete!\n"
     ]
    }
   ],
   "source": [
    "# Test LLM labeling on the top 3 topics first\n",
    "print(\"Generating LLM labels for top 3 topics...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, topic in enumerate(final_topics_improved[:3], 1):\n",
    "    print(f\"\\nüè∑Ô∏è  Labeling Topic {topic['topic_id']} (Size: {topic['size']})...\")\n",
    "    \n",
    "    try:\n",
    "        label = label_topic_with_llm(topic, client)\n",
    "        topic[\"llm_label\"] = label\n",
    "        print(f\"\\n{label}\")\n",
    "        print(\"-\"*80)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error labeling topic {topic['topic_id']}: {e}\")\n",
    "        topic[\"llm_label\"] = None\n",
    "\n",
    "print(\"\\n‚úÖ Test labeling complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b413101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating LLM labels for ALL 12 topics...\n",
      "This may take a few minutes...\n",
      "\n",
      "[4/12] Labeling Topic 10 (Size: 567)... ‚úÖ\n",
      "[5/12] Labeling Topic 4 (Size: 480)... ‚úÖ\n",
      "[6/12] Labeling Topic 9 (Size: 392)... ‚úÖ\n",
      "[7/12] Labeling Topic 7 (Size: 296)... ‚úÖ\n",
      "[8/12] Labeling Topic 5 (Size: 146)... ‚úÖ\n",
      "[9/12] Labeling Topic 6 (Size: 139)... ‚úÖ\n",
      "[10/12] Labeling Topic 0 (Size: 91)... ‚úÖ\n",
      "[11/12] Labeling Topic 3 (Size: 63)... ‚úÖ\n",
      "[12/12] Labeling Topic 1 (Size: 61)... ‚úÖ\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Labeling complete!\n",
      "   Successfully labeled: 12/12\n"
     ]
    }
   ],
   "source": [
    "# Label ALL topics (this may take a few minutes)\n",
    "import time\n",
    "\n",
    "print(f\"Generating LLM labels for ALL {len(final_topics_improved)} topics...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "labeled_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for i, topic in enumerate(final_topics_improved, 1):\n",
    "    # Skip if already labeled\n",
    "    if \"llm_label\" in topic and topic[\"llm_label\"]:\n",
    "        labeled_count += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"[{i}/{len(final_topics_improved)}] Labeling Topic {topic['topic_id']} (Size: {topic['size']})...\", end=\" \")\n",
    "        label = label_topic_with_llm(topic, client)\n",
    "        topic[\"llm_label\"] = label\n",
    "        labeled_count += 1\n",
    "        print(\"‚úÖ\")\n",
    "        \n",
    "        # Small delay to avoid rate limits\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        topic[\"llm_label\"] = None\n",
    "        error_count += 1\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Labeling complete!\")\n",
    "print(f\"   Successfully labeled: {labeled_count}/{len(final_topics_improved)}\")\n",
    "if error_count > 0:\n",
    "    print(f\"   Errors: {error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0b4b3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL LABELED TOPICS\n",
      "================================================================================\n",
      "\n",
      "üî¥ MACRO-TOPIC\n",
      "üìä TOPIC 11 | Size: 3102 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic Label  \n",
      "Security, Justice, and Governance in Michoac√°n\n",
      "\n",
      "2. Brief Description  \n",
      "This topic centers on issues of public security and justice in Michoac√°n, highlighting government strategies to combat crime, strengthen institutions, and address extortion affecting local industries. It emphasizes collective governmental action, extradition of criminals, and the need for robust law enforcement and judicial systems.\n",
      "\n",
      "3. Macro-topic Explanation  \n",
      "As a macro-topic, this functions as a narrative backbone because it integrates core themes of governance, security, and justice that underpin broader political discourse. It connects specific regional concerns (Michoac√°n, extortion, local industries) with national strategies and institutional reforms, serving as a foundation for related discussions on policy, leadership, and public trust.\n",
      "\n",
      "üîë Top Keywords: seguridad, pueblo, michoac√°n, hemos, muchas\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 8 | Size: 1203 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic Label: Road Infrastructure and Emergency Response\n",
      "\n",
      "2. Description: This topic centers on government efforts to restore and maintain roadways and bridges, particularly in response to emergencies such as heavy rains. It highlights coordination among federal and state officials, with specific references to regions like Hidalgo and Veracruz, and emphasizes ongoing work and support for affected areas.\n",
      "\n",
      "3. Macro-topic Status: This is a regular topic, not a macro-topic.\n",
      "\n",
      "üîë Top Keywords: caminos, mil, trabajando, son, va\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 2 | Size: 645 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic Label: Public Healthcare Access and Medication\n",
      "\n",
      "2. Description:  \n",
      "This topic centers on government efforts to improve healthcare infrastructure, ensure access to free and quality medical attention, and rationalize the distribution of essential medications. It highlights initiatives such as telemedicine, national health programs, and outreach to rural and underserved communities.\n",
      "\n",
      "3. Macro-topic Explanation:  \n",
      "This is a regular topic, not a macro-topic.\n",
      "\n",
      "üîë Top Keywords: medicamentos, bienestar, atenci√≥n, imss bienestar, va\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 10 | Size: 567 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. **Concise topic label:**  \n",
      "Mexico‚ÄìUS Relations and Agreements\n",
      "\n",
      "2. **Brief description:**  \n",
      "This topic centers on discussions regarding diplomatic negotiations and agreements between Mexico and the United States, particularly focusing on non-tariff measures, sovereignty, and economic cooperation. It references high-level meetings, the defense of national interests, and concerns about political or business motivations influencing bilateral decisions.\n",
      "\n",
      "3. **Macro-topic explanation:**  \n",
      "This is a regular topic, not a macro-topic.\n",
      "\n",
      "üîë Top Keywords: va, tema, bueno, acuerdo, nosotros\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 4 | Size: 480 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic Label: Expanding Higher Education Access\n",
      "\n",
      "2. Description:  \n",
      "This topic centers on efforts to increase opportunities for young people to access higher education in Mexico. It discusses expanding university spaces, democratizing knowledge through platforms, and addressing the demand for more educational institutions across the country.\n",
      "\n",
      "3. Macro-topic Explanation:  \n",
      "This is a regular topic, not a macro-topic, as it focuses specifically on higher education access rather than serving as a broader narrative framework.\n",
      "\n",
      "üîë Top Keywords: superior, j√≥venes, mil, universidades, educaci√≥n superior\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 9 | Size: 392 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic label: Water Law and Concessions Reform\n",
      "\n",
      "2. Description:  \n",
      "This topic centers on legislative and policy discussions regarding water rights, the regulation of water concessions, and the management of irrigation resources in Mexico. It highlights debates about returning water control to the state, reducing privileges, and ensuring equitable distribution, especially in the context of agricultural use.\n",
      "\n",
      "3. Macro-topic explanation:  \n",
      "This is a regular topic, not a macro-topic.\n",
      "\n",
      "üîë Top Keywords: ley, riego, conagua, aguas, va\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 7 | Size: 296 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Concise topic label (3‚Äì6 words):  \n",
      "2026 FIFA World Cup Host Cities\n",
      "\n",
      "2. Brief description:  \n",
      "This topic covers discussions about preparations and activities related to Mexico hosting matches for the upcoming FIFA World Cup. It highlights the involvement of key cities‚ÄîMexico City, Guadalajara, and Monterrey‚Äîin organizing events, infrastructure, and social initiatives to maximize the benefits of the tournament.\n",
      "\n",
      "3. Macro-topic explanation:  \n",
      "This is a regular topic, not a macro-topic.\n",
      "\n",
      "üîë Top Keywords: pa√≠s, fifa, futbol, ciudad, van\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 5 | Size: 146 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic label: Agricultural Support Programs and Prices\n",
      "\n",
      "2. Description: This topic centers on government programs providing financial and material support to small producers, especially in the maize sector. It discusses initiatives like Sembrando Vida and Producci√≥n para el Bienestar, their budgets, and issues related to fair pricing and market conditions for agricultural products.\n",
      "\n",
      "3. Macro-topic explanation: Not applicable; this is a regular topic.\n",
      "\n",
      "üîë Top Keywords: productores, precio, va, mil, programa\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 6 | Size: 139 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic label: Housing Programs and Credits\n",
      "\n",
      "2. Description:  \n",
      "This topic centers on government housing initiatives, specifically the allocation and contracting of homes through programs like FOVISSSTE. It highlights the importance of these programs for beneficiaries, the process of acquiring housing credits, and the emotional impact on recipients.\n",
      "\n",
      "3. Macro-topic explanation:  \n",
      "This is a regular topic, not a macro-topic.\n",
      "\n",
      "üîë Top Keywords: viviendas, mil viviendas, programa, meta, fovissste\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 0 | Size: 91 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic Label: Gasoline Prices and Consumer Monitoring\n",
      "\n",
      "2. Description:  \n",
      "This topic centers on the monitoring and reporting of gasoline prices, including comparisons of price averages, margins of profit, and the identification of stations with notably high or low prices. It also references consumer tools and advice provided by Profeco to help individuals make informed purchasing decisions.\n",
      "\n",
      "3. Macro-topic Explanation:  \n",
      "This is a regular topic, not a macro-topic, as it focuses specifically on gasoline pricing and consumer information rather than serving as a broader narrative framework.\n",
      "\n",
      "üîë Top Keywords: d√≥lares, centavos, mil, da, barata\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 3 | Size: 63 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic Label: National Supercomputer Project\n",
      "\n",
      "2. Description:  \n",
      "This topic centers on the development and presentation of \"Coatlicue,\" a major supercomputer initiative in Mexico. It highlights the project's anticipated impact on scientific and technological capacity, public benefits in areas like health and fiscal oversight, and the collaborative management between government institutions.\n",
      "\n",
      "3. Macro-topic Explanation:  \n",
      "This is a regular topic, not a macro-topic.\n",
      "\n",
      "üîë Top Keywords: supercomputadora, datos, centro, superc√≥mputo, va\n",
      "================================================================================\n",
      "\n",
      "üü¢ Regular Topic\n",
      "üìä TOPIC 1 | Size: 61 speeches\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Topic Label: Promoting Reading and Books\n",
      "\n",
      "2. Description:  \n",
      "This topic centers on initiatives to distribute books, encourage reading, and foster a culture of literacy, especially among young people. It discusses the impact of reading on personal development and contrasts it with the influence of social media.\n",
      "\n",
      "3. Macro-topic Explanation:  \n",
      "This is a regular topic, not a macro-topic.\n",
      "\n",
      "üîë Top Keywords: libro, leer, 25, va, lectura\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display all labeled topics with LLM-generated descriptions\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL LABELED TOPICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for topic in final_topics_improved:\n",
    "    topic_type = \"üî¥ MACRO-TOPIC\" if topic[\"is_macro_topic\"] else \"üü¢ Regular Topic\"\n",
    "    \n",
    "    print(f\"\\n{topic_type}\")\n",
    "    print(f\"üìä TOPIC {topic['topic_id']} | Size: {topic['size']} speeches\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if topic.get(\"llm_label\"):\n",
    "        print(f\"\\n{topic['llm_label']}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No LLM label generated\")\n",
    "    \n",
    "    print(f\"\\nüîë Top Keywords: {', '.join(topic['keywords'][:5])}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3fc78cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 12 labeled topics to topics_labeled_q4_2025.json\n",
      "   ‚Ä¢ 1 macro-topics\n",
      "   ‚Ä¢ 11 regular topics\n"
     ]
    }
   ],
   "source": [
    "# Save labeled topics to JSON file for later use\n",
    "output_data = []\n",
    "\n",
    "for topic in final_topics_improved:\n",
    "    output_data.append({\n",
    "        \"topic_id\": int(topic[\"topic_id\"]),\n",
    "        \"size\": int(topic[\"size\"]),\n",
    "        \"is_macro_topic\": bool(topic[\"is_macro_topic\"]),\n",
    "        \"keywords\": topic[\"keywords\"],\n",
    "        \"llm_label\": topic.get(\"llm_label\", \"\"),\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"text\": ex[\"text\"],\n",
    "                \"speaker\": ex[\"speaker_normalized\"],\n",
    "                \"date\": ex[\"published_at\"].isoformat() if hasattr(ex[\"published_at\"], \"isoformat\") else str(ex[\"published_at\"]),\n",
    "                \"token_count\": int(ex.get(\"token_count\", 0))\n",
    "            }\n",
    "            for ex in topic[\"examples\"]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "output_file = \"topics_labeled_q4_2025.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(output_data)} labeled topics to {output_file}\")\n",
    "print(f\"   ‚Ä¢ {sum(1 for t in output_data if t['is_macro_topic'])} macro-topics\")\n",
    "print(f\"   ‚Ä¢ {sum(1 for t in output_data if not t['is_macro_topic'])} regular topics\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
